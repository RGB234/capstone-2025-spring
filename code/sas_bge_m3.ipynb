{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHWrFa_jIFKw"
      },
      "source": [
        "## Customized BGE-M3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FUgVH4wvYukn"
      },
      "outputs": [],
      "source": [
        "# from transformers import TFAutoModel, TFPreTrainedModel, PretrainedConfig\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import tensorflow as tf\n",
        "\n",
        "# class SentenceMultiHeadAttention(tf.keras.layers.Layer):\n",
        "#   def __init__(self, d_model, num_heads, name=\"sentence_multi_head_attention\"):\n",
        "#     super(SentenceMultiHeadAttention, self).__init__(name=name)\n",
        "#     self.d_model = d_model\n",
        "#     self.num_heads = num_heads\n",
        "\n",
        "#     assert d_model % num_heads == 0\n",
        "\n",
        "#     # transformer 논문 기준 512 // 8\n",
        "#     self.depth = d_model // num_heads\n",
        "\n",
        "#     # WQ, WK, WV\n",
        "#     self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
        "#     self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
        "#     self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
        "\n",
        "#     # W0\n",
        "#     self.dense = tf.keras.layers.Dense(units=d_model)\n",
        "\n",
        "\n",
        "#   def split_heads(self, inputs):\n",
        "#     # (문장 갯수(bge-m3 의 batch_size), 모델 차원 d_model) -> (문장 갯수, num_heads, depth)\n",
        "#     inputs = tf.reshape(inputs, shape=[-1, self.num_heads, self.depth])\n",
        "#     return tf.transpose(inputs, perm=[1, 0, 2])\n",
        "\n",
        "\n",
        "#   def scaled_dot_product(self, query, key, value):\n",
        "#     # query, key, value : (num_heads, batch_size 문장갯수, d_model/num_heads)\n",
        "#     # 문장 임베딩 끼리의 내적이므로 padding mask 는 필요없음\n",
        "#     mat_mul = tf.matmul(query, key, transpose_b=True)\n",
        "#     # dk = d_model/num_heads\n",
        "#     dk = tf.cast(tf.shape(key)[-1], tf.float32)\n",
        "#     logits = tf.divide(mat_mul, tf.math.sqrt(dk))\n",
        "\n",
        "#     attention_weights = tf.nn.softmax(logits, axis=-1) # key 방향\n",
        "#     output = tf.matmul(attention_weights, value)\n",
        "\n",
        "#     return output, attention_weights\n",
        "\n",
        "#   def call(self, input):\n",
        "#     # query, key, value : (batch_size, d_model)\n",
        "#     query, key, value = input['query'], input['key'], input['value']\n",
        "\n",
        "#     # WQ\n",
        "#     query = self.query_dense(query)\n",
        "#     # WK\n",
        "#     key = self.key_dense(key)\n",
        "#     # WV\n",
        "#     value = self.value_dense(value)\n",
        "\n",
        "#     # -> (batch_size, num_heads, d_model/num_heads)\n",
        "#     query = self.split_heads(query)\n",
        "#     key = self.split_heads(key)\n",
        "#     value = self.split_heads(value)\n",
        "\n",
        "#     # (num_heads, batch_size, d_model / num_heads)\n",
        "#     scaled_attention, _ = self.scaled_dot_product(query, key, value)\n",
        "#     # (batch_size, num_heads, d_model / num_heads)\n",
        "#     scaled_attention = tf.transpose(scaled_attention, perm=[1, 0, 2])\n",
        "\n",
        "#     # (batch_size, d_model)\n",
        "#     concat_attention = tf.reshape(scaled_attention, (-1, self.d_model))\n",
        "\n",
        "#     # W0\n",
        "#     outputs = self.dense(concat_attention)\n",
        "\n",
        "#     return outputs\n",
        "\n",
        "# # self attention of sentences\n",
        "# class CustomBGEM3FlagModel(TFPreTrainedModel):\n",
        "#   config_class = PretrainedConfig\n",
        "#   def __init__(self,config):\n",
        "#     super().__init__(config)\n",
        "#     self.config = config\n",
        "#     self.bge_m3 = TFAutoModel.from_pretrained(\"BAAI/bge-m3\", from_pt=True)\n",
        "#     self.dff = config.intermediate_size # 4096\n",
        "#     # self.dff = 2048\n",
        "#     self.d_model = 1024\n",
        "#     self.num_heads = config.num_attention_heads # 16\n",
        "#     self.dropout = config.hidden_dropout_prob # 0.1\n",
        "\n",
        "#     # the additional layer\n",
        "#     self.clustering_layer = self.sentence_clustering_layer(dff=self.dff, d_model=self.d_model, num_heads=self.num_heads, dropout=self.dropout)\n",
        "\n",
        "#   def call(self,inputs, training=False):\n",
        "#     # sequence_output : [batch_size, sequence_length, the dimension of bge-m3]\n",
        "#     ids, attention_mask = inputs\n",
        "#     # (batch_size(문장 수), 문장 길이(토큰 수), 모델 차원 d_model)\n",
        "#     sequence_output = self.bge_m3(ids, attention_mask=attention_mask, training=training).last_hidden_state\n",
        "\n",
        "#     # (batch_size, d_model)\n",
        "#     # extract the 1st token's ([CLS]) embeddings\n",
        "#     sentence_embeddings = sequence_output[:, 0, :]\n",
        "\n",
        "#     output = self.clustering_layer(sentence_embeddings)\n",
        "#     return output\n",
        "\n",
        "#   def sentence_clustering_layer(self, dff, d_model, num_heads, dropout, name=\"bert_layer\"):\n",
        "#     # (문장 갯수 batch_size, d_model)\n",
        "#     inputs = tf.keras.Input(shape=(d_model,), name=\"input\")\n",
        "\n",
        "#     # sub-layer 1 : Multi head self attention\n",
        "#     attention = SentenceMultiHeadAttention(d_model, num_heads, name=\"sentence_multi_head_attention\")({\n",
        "#         'query' : inputs,\n",
        "#         'key' : inputs,\n",
        "#         'value' : inputs,\n",
        "#     })\n",
        "\n",
        "#     # drop-out\n",
        "#     attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
        "#     # 잔차연결, 층 정규화\n",
        "#     attention = tf.keras.layers.LayerNormalization(epsilon=1e-6)(inputs + attention)\n",
        "\n",
        "#     # sub-layer 2 : Position-wide FFNN\n",
        "#     outputs = tf.keras.layers.Dense(units=dff, activation='relu')(attention)\n",
        "#     outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
        "\n",
        "#     # drop-out\n",
        "#     outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
        "#     # 잔차연결, 층 정규화\n",
        "#     outputs = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attention + outputs)\n",
        "\n",
        "#     return tf.keras.Model(inputs=[inputs], outputs=outputs, name=name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMrO_T1TgvOr"
      },
      "source": [
        "Pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NncKA9U-gt21"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModel, PreTrainedModel, PretrainedConfig\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class SentenceMultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(SentenceMultiHeadAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        assert d_model % num_heads == 0\n",
        "        self.depth = d_model // num_heads\n",
        "\n",
        "        self.query_dense = nn.Linear(d_model, d_model)\n",
        "        self.key_dense = nn.Linear(d_model, d_model)\n",
        "        self.value_dense = nn.Linear(d_model, d_model)\n",
        "        self.out_dense = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        # (batch_size, d_model) → (batch_size, num_heads, depth)\n",
        "        x = x.view(-1, self.num_heads, self.depth)\n",
        "        return x.transpose(0, 1)  # (num_heads, batch_size, depth)\n",
        "\n",
        "    def scaled_dot_product(self, query, key, value):\n",
        "        dk = query.size(-1)\n",
        "        scores = torch.matmul(query, key.transpose(-2, -1)) / (dk ** 0.5)\n",
        "        weights = F.softmax(scores, dim=-1)\n",
        "        output = torch.matmul(weights, value)\n",
        "        return output, weights\n",
        "\n",
        "    def forward(self, query, key, value):\n",
        "        # Input: (batch_size, d_model)\n",
        "        query = self.query_dense(query)\n",
        "        key = self.key_dense(key)\n",
        "        value = self.value_dense(value)\n",
        "\n",
        "        query = self.split_heads(query)\n",
        "        key = self.split_heads(key)\n",
        "        value = self.split_heads(value)\n",
        "\n",
        "        scaled_attention, _ = self.scaled_dot_product(query, key, value)\n",
        "\n",
        "        # (num_heads, batch_size, depth) → (batch_size, num_heads, depth)\n",
        "        scaled_attention = scaled_attention.transpose(0, 1)\n",
        "        concat_attention = scaled_attention.reshape(-1, self.d_model)\n",
        "\n",
        "        output = self.out_dense(concat_attention)\n",
        "        return output\n",
        "\n",
        "\n",
        "class SentenceClusteringLayer(nn.Module):\n",
        "    def __init__(self, dff, d_model, num_heads, dropout):\n",
        "        super(SentenceClusteringLayer, self).__init__()\n",
        "        self.attention = SentenceMultiHeadAttention(d_model, num_heads)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(d_model, dff),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(dff, d_model),\n",
        "        )\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch_size, d_model)\n",
        "        attn_output = self.attention(x, x, x)\n",
        "        attn_output = self.dropout1(attn_output)\n",
        "        out1 = self.norm1(x + attn_output)\n",
        "\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output)\n",
        "        out2 = self.norm2(out1 + ffn_output)\n",
        "        return out2\n",
        "\n",
        "\n",
        "class CustomBGEM3FlagModel(PreTrainedModel):\n",
        "    config_class = PretrainedConfig\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.config = config\n",
        "        self.d_model = 1024\n",
        "        self.dff = config.intermediate_size\n",
        "        self.num_heads = config.num_attention_heads\n",
        "        self.dropout = config.hidden_dropout_prob\n",
        "\n",
        "        self.bge_m3 = AutoModel.from_pretrained(\"BAAI/bge-m3\")\n",
        "\n",
        "        self.clustering_layer = SentenceClusteringLayer(\n",
        "            dff=self.dff,\n",
        "            d_model=self.d_model,\n",
        "            num_heads=self.num_heads,\n",
        "            dropout=self.dropout\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        # input_ids: [batch_size, seq_len]\n",
        "        output = self.bge_m3(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        sequence_output = output.last_hidden_state  # [batch_size, seq_len, d_model]\n",
        "\n",
        "        sentence_embeddings = sequence_output[:, 0, :]  # [CLS] token\n",
        "        out = self.clustering_layer(sentence_embeddings)  # [batch_size, d_model]\n",
        "\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "cb1162bae5ff4dd28c3b7509b3c88487",
            "36eb342e53ae42e9b9d52355c964a3ba",
            "ebfa375e1e29436c923224a43bbc644e",
            "d24c903525644222a010c8c032ac0a5c",
            "a461f23bcd5043298b7fa46f60f176db",
            "8e4b83088783471895de80ca854ba3a3",
            "5ae6a9b72dcd4686b6a0d7940854f27e",
            "f398386ec7d64865866daa3c236d98a9",
            "411232de05644b1789aeb63547291e41",
            "5718475a10794634bcceafdc8a3214ae",
            "f0c39d85d5e143e7bf86a6bce1135ef0"
          ]
        },
        "id": "59eDWkrultiO",
        "outputId": "46ec0eff-82c9-4be9-a214-0a46749b8eee"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are using a model of type xlm-roberta to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cb1162bae5ff4dd28c3b7509b3c88487",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.27G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "config = PretrainedConfig.from_pretrained(\"BAAI/bge-m3\") # 그대로 사용\n",
        "model = CustomBGEM3FlagModel(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l41h1kHx-J-Y",
        "outputId": "9404b35d-06cd-46cb-f3b2-6f29d7799be8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('./sas-bge-m3/tokenizer_config.json',\n",
              " './sas-bge-m3/special_tokens_map.json',\n",
              " './sas-bge-m3/sentencepiece.bpe.model',\n",
              " './sas-bge-m3/added_tokens.json',\n",
              " './sas-bge-m3/tokenizer.json')"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "path = \"./sas-bge-m3\"\n",
        "\n",
        "model.save_pretrained(path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-m3\")\n",
        "tokenizer.save_pretrained(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "WUIAxaQV_f4N"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# model_path = \"./sas-bge-m3\"\n",
        "# config = PretrainedConfig.from_pretrained(model_path)\n",
        "# custom_model = CustomBGEM3FlagModel.from_pretrained(model_path, config=config, from_pt=True)\n",
        "\n",
        "# custom_model.summary()\n",
        "\n",
        "model = CustomBGEM3FlagModel.from_pretrained(path) # pytorch\n",
        "tokenizer = AutoTokenizer.from_pretrained(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJeCRkLmhhpv",
        "outputId": "828b7cea-401a-49ad-93be-38e096d7c56e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 0.7489,  0.4860, -2.1363,  ..., -0.2958, -0.1386,  0.3238],\n",
              "        [ 0.9537,  0.5229, -2.3019,  ..., -1.0513,  0.4561,  0.7512]],\n",
              "       grad_fn=<NativeLayerNormBackward0>)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "texts = [\"아버지 가방에 들어가십니다.\", \"아버지가 방에 들어가십니다.\"]\n",
        "inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\") # pytorch\n",
        "model(**inputs)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "36eb342e53ae42e9b9d52355c964a3ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e4b83088783471895de80ca854ba3a3",
            "placeholder": "​",
            "style": "IPY_MODEL_5ae6a9b72dcd4686b6a0d7940854f27e",
            "value": "model.safetensors: 100%"
          }
        },
        "411232de05644b1789aeb63547291e41": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5718475a10794634bcceafdc8a3214ae": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ae6a9b72dcd4686b6a0d7940854f27e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8e4b83088783471895de80ca854ba3a3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a461f23bcd5043298b7fa46f60f176db": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb1162bae5ff4dd28c3b7509b3c88487": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_36eb342e53ae42e9b9d52355c964a3ba",
              "IPY_MODEL_ebfa375e1e29436c923224a43bbc644e",
              "IPY_MODEL_d24c903525644222a010c8c032ac0a5c"
            ],
            "layout": "IPY_MODEL_a461f23bcd5043298b7fa46f60f176db"
          }
        },
        "d24c903525644222a010c8c032ac0a5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5718475a10794634bcceafdc8a3214ae",
            "placeholder": "​",
            "style": "IPY_MODEL_f0c39d85d5e143e7bf86a6bce1135ef0",
            "value": " 2.27G/2.27G [00:22&lt;00:00, 110MB/s]"
          }
        },
        "ebfa375e1e29436c923224a43bbc644e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f398386ec7d64865866daa3c236d98a9",
            "max": 2271064456,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_411232de05644b1789aeb63547291e41",
            "value": 2271064456
          }
        },
        "f0c39d85d5e143e7bf86a6bce1135ef0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f398386ec7d64865866daa3c236d98a9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
