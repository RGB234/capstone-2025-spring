{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5da363f6",
   "metadata": {},
   "source": [
    "negative-mining.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed457394",
   "metadata": {},
   "source": [
    "train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413e1181",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/bash\n",
    "#SBATCH -J bge-m3\n",
    "#SBATCH --gres=gpu:1\n",
    "#SBATCH --cpus-per-gpu=6\n",
    "#SBATCH --mem-per-gpu=20G\n",
    "#SBATCH -w aurora-g3\n",
    "#SBATCH -p batch_ugrad\n",
    "#SBATCH -t 1-0\n",
    "#SBATCH -o logs/slurm-%A.out\n",
    "  \n",
    "pwd\n",
    "which python\n",
    "hostname\n",
    "\n",
    "python hn_mine.py \\\n",
    "--embedder_name_or_path BAAI/bge-m3 \\\n",
    "--input_file /data2/local_datasets/bge-m3/ft_data/relevant_incidents.jsonl \\\n",
    "--output_file /data2/local_datasets/bge-m3/ft_data/relevant_incidents_minedHN.jsonl \\\n",
    "--range_for_sampling 2-300 \\\n",
    "--negative_number 32 \\\n",
    "--use_gpu_for_searching \n",
    "\n",
    "exit 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00836913",
   "metadata": {},
   "source": [
    "test (or validation) set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d2f3c9",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/bash\n",
    "#SBATCH -J bge-m3\n",
    "#SBATCH --gres=gpu:1\n",
    "#SBATCH --cpus-per-gpu=8\n",
    "#SBATCH --mem-per-gpu=32G\n",
    "#SBATCH -w aurora-g8\n",
    "#SBATCH -p batch_ugrad\n",
    "#SBATCH -t 1-0\n",
    "#SBATCH -o logs/slurm-%A.out\n",
    "\n",
    "pwd\n",
    "\n",
    "python hn_mine.py \\\n",
    "--embedder_name_or_path BAAI/bge-m3 \\\n",
    "--input_file /data2/local_datasets/bge-m3/data/relevant_incidents_test.jsonl \\\n",
    "--output_file /data2/local_datasets/bge-m3/data/relevant_incidents_test_minedHN.jsonl \\\n",
    "--range_for_sampling 2-300 \\\n",
    "--negative_number 8 \\\n",
    "--use_gpu_for_searching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4024cbf7",
   "metadata": {},
   "source": [
    "hn_mine.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6797e4f",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# copied from https://github.com/FlagOpen/FlagEmbedding/blob/master/scripts/hn_mine.py\n",
    "\n",
    "# MIT License\n",
    "\n",
    "# Copyright (c) 2022 staoxiao\n",
    "\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "# of this software and associated documentation files (the \"Software\"), to deal\n",
    "# in the Software without restriction, including without limitation the rights\n",
    "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "# copies of the Software, and to permit persons to whom the Software is\n",
    "# furnished to do so, subject to the following conditions:\n",
    "\n",
    "# The above copyright notice and this permission notice shall be included in all\n",
    "# copies or substantial portions of the Software.\n",
    "\n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "# SOFTWARE.\n",
    "\n",
    "\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from typing import Optional\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "import faiss\n",
    "from transformers import HfArgumentParser\n",
    "from FlagEmbedding import FlagAutoModel\n",
    "from FlagEmbedding.abc.inference import AbsEmbedder\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataArgs:\n",
    "    \"\"\"\n",
    "    Data arguments for hard negative mining.\n",
    "    \"\"\"\n",
    "\n",
    "    input_file: str = field(\n",
    "        metadata={\"help\": \"The input file for hard negative mining.\"}\n",
    "    )\n",
    "    output_file: str = field(\n",
    "        metadata={\"help\": \"The output file for hard negative mining.\"}\n",
    "    )\n",
    "    candidate_pool: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"The candidate pool for hard negative mining. If provided, it should be a jsonl file, each line is a dict with a key 'text'.\"\n",
    "        },\n",
    "    )\n",
    "    range_for_sampling: str = field(\n",
    "        default=\"10-210\", metadata={\"help\": \"The range to sample negatives.\"}\n",
    "    )\n",
    "    negative_number: int = field(\n",
    "        default=15, metadata={\"help\": \"The number of negatives.\"}\n",
    "    )\n",
    "    use_gpu_for_searching: bool = field(\n",
    "        default=False, metadata={\"help\": \"Whether to use faiss-gpu for searching.\"}\n",
    "    )\n",
    "    search_batch_size: int = field(\n",
    "        default=64, metadata={\"help\": \"The batch size for searching.\"}\n",
    "    )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelArgs:\n",
    "    \"\"\"\n",
    "    Model arguments for embedder.\n",
    "    \"\"\"\n",
    "\n",
    "    embedder_name_or_path: str = field(\n",
    "        metadata={\"help\": \"The embedder name or path.\", \"required\": True}\n",
    "    )\n",
    "    embedder_model_class: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"The embedder model class. Available classes: ['encoder-only-base', 'encoder-only-m3', 'decoder-only-base', 'decoder-only-icl']. Default: None. For the custom model, you need to specifiy the model class.\",\n",
    "            \"choices\": [\n",
    "                \"encoder-only-base\",\n",
    "                \"encoder-only-m3\",\n",
    "                \"decoder-only-base\",\n",
    "                \"decoder-only-icl\",\n",
    "            ],\n",
    "        },\n",
    "    )\n",
    "    normalize_embeddings: bool = field(\n",
    "        default=True, metadata={\"help\": \"whether to normalize the embeddings\"}\n",
    "    )\n",
    "    pooling_method: str = field(\n",
    "        default=\"cls\", metadata={\"help\": \"The pooling method fot the embedder.\"}\n",
    "    )\n",
    "    use_fp16: bool = field(\n",
    "        default=True, metadata={\"help\": \"whether to use fp16 for inference\"}\n",
    "    )\n",
    "    devices: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Devices to use for inference.\", \"nargs\": \"+\"}\n",
    "    )\n",
    "    query_instruction_for_retrieval: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Instruction for query\"}\n",
    "    )\n",
    "    query_instruction_format_for_retrieval: str = field(\n",
    "        default=\"{}{}\", metadata={\"help\": \"Format for query instruction\"}\n",
    "    )\n",
    "    examples_for_task: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Examples for task\"}\n",
    "    )\n",
    "    examples_instruction_format: str = field(\n",
    "        default=\"{}{}\", metadata={\"help\": \"Format for examples instruction\"}\n",
    "    )\n",
    "    trust_remote_code: bool = field(\n",
    "        default=False, metadata={\"help\": \"Trust remote code\"}\n",
    "    )\n",
    "    cache_dir: str = field(\n",
    "        default=None, metadata={\"help\": \"Cache directory for models.\"}\n",
    "    )\n",
    "    # ================ for inference ===============\n",
    "    batch_size: int = field(\n",
    "        default=3000, metadata={\"help\": \"Batch size for inference.\"}\n",
    "    )\n",
    "    embedder_query_max_length: int = field(\n",
    "        default=512, metadata={\"help\": \"Max length for query.\"}\n",
    "    )\n",
    "    embedder_passage_max_length: int = field(\n",
    "        default=512, metadata={\"help\": \"Max length for passage.\"}\n",
    "    )\n",
    "\n",
    "    def __post_init__(self):\n",
    "        # replace \"\\\\n\" with \"\\n\"\n",
    "        if \"\\\\n\" in self.query_instruction_format_for_retrieval:\n",
    "            self.query_instruction_format_for_retrieval = (\n",
    "                self.query_instruction_format_for_retrieval.replace(\"\\\\n\", \"\\n\")\n",
    "            )\n",
    "        if \"\\\\n\" in self.examples_instruction_format:\n",
    "            self.examples_instruction_format = self.examples_instruction_format.replace(\n",
    "                \"\\\\n\", \"\\n\"\n",
    "            )\n",
    "\n",
    "\n",
    "def create_index(embeddings: np.ndarray, use_gpu: bool = False):\n",
    "    index = faiss.IndexFlatIP(len(embeddings[0]))\n",
    "    embeddings = np.asarray(embeddings, dtype=np.float32)\n",
    "    if use_gpu:\n",
    "        co = faiss.GpuMultipleClonerOptions()\n",
    "        co.shard = True\n",
    "        co.useFloat16 = True\n",
    "        index = faiss.index_cpu_to_all_gpus(index, co=co)\n",
    "    index.add(embeddings)\n",
    "    return index\n",
    "\n",
    "\n",
    "def batch_search(\n",
    "    index: faiss.Index, query: np.ndarray, topk: int = 200, batch_size: int = 64\n",
    "):\n",
    "    all_scores, all_inxs = [], []\n",
    "    for start_index in tqdm(\n",
    "        range(0, len(query), batch_size), desc=\"Batches\", disable=len(query) < 256\n",
    "    ):\n",
    "        batch_query = query[start_index : start_index + batch_size]\n",
    "        batch_scores, batch_inxs = index.search(\n",
    "            np.asarray(batch_query, dtype=np.float32), k=topk\n",
    "        )\n",
    "        all_scores.extend(batch_scores.tolist())\n",
    "        all_inxs.extend(batch_inxs.tolist())\n",
    "    return all_scores, all_inxs\n",
    "\n",
    "\n",
    "def get_corpus(candidate_pool: str):\n",
    "    corpus = []\n",
    "    with open(candidate_pool, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f.readlines():\n",
    "            line = json.loads(line.strip())\n",
    "            corpus.append(line[\"text\"])\n",
    "    return corpus\n",
    "\n",
    "\n",
    "def find_knn_neg(\n",
    "    model: AbsEmbedder,\n",
    "    input_file: str,\n",
    "    output_file: str,\n",
    "    candidate_pool: Optional[str] = None,\n",
    "    sample_range: str = \"10-210\",\n",
    "    negative_number: int = 15,\n",
    "    use_gpu: bool = False,\n",
    "):\n",
    "    corpus = []\n",
    "    queries = []\n",
    "    train_data = []\n",
    "    for line in open(input_file):\n",
    "        line = json.loads(line.strip())\n",
    "        train_data.append(line)\n",
    "        corpus.extend(line[\"pos\"])\n",
    "        if \"neg\" in line:\n",
    "            corpus.extend([line[\"neg\"]])\n",
    "        queries.append(line[\"query\"])\n",
    "\n",
    "    if candidate_pool is not None:\n",
    "        if not isinstance(candidate_pool, list):\n",
    "            candidate_pool = get_corpus(candidate_pool)\n",
    "        corpus = list(set(candidate_pool))\n",
    "    else:\n",
    "        corpus = list(set(corpus))\n",
    "\n",
    "    print(f\"inferencing embedding for corpus (number={len(corpus)})--------------\")\n",
    "    p_vecs = model.encode(corpus)\n",
    "    print(f\"inferencing embedding for queries (number={len(queries)})--------------\")\n",
    "    q_vecs = model.encode_queries(queries)\n",
    "\n",
    "    # check if the embeddings are in dictionary format: M3Embedder\n",
    "    if isinstance(p_vecs, dict):\n",
    "        p_vecs = p_vecs[\"dense_vecs\"]\n",
    "    if isinstance(q_vecs, dict):\n",
    "        q_vecs = q_vecs[\"dense_vecs\"]\n",
    "\n",
    "    print(\"create index and search------------------\")\n",
    "    index = create_index(p_vecs, use_gpu=use_gpu)\n",
    "    _, all_inxs = batch_search(index, q_vecs, topk=sample_range[-1])\n",
    "    assert len(all_inxs) == len(train_data)\n",
    "\n",
    "    for i, data in enumerate(train_data):\n",
    "        query = data[\"query\"]\n",
    "        inxs = all_inxs[i][sample_range[0] : sample_range[1]]\n",
    "        filtered_inx = []\n",
    "        for inx in inxs:\n",
    "            if inx == -1:\n",
    "                break\n",
    "            if corpus[inx] not in data[\"pos\"] and corpus[inx] != query:\n",
    "                filtered_inx.append(inx)\n",
    "\n",
    "        if len(filtered_inx) > negative_number:\n",
    "            filtered_inx = random.sample(filtered_inx, negative_number)\n",
    "        data[\"neg\"] = [corpus[inx] for inx in filtered_inx]\n",
    "\n",
    "    with open(output_file, \"w\") as f:\n",
    "        for data in train_data:\n",
    "            if len(data[\"neg\"]) < negative_number:\n",
    "                samples = random.sample(\n",
    "                    corpus, negative_number - len(data[\"neg\"]) + len(data[\"pos\"])\n",
    "                )\n",
    "                samples = [sent for sent in samples if sent not in data[\"pos\"]]\n",
    "                data[\"neg\"].extend(samples[: negative_number - len(data[\"neg\"])])\n",
    "            f.write(json.dumps(data, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "\n",
    "def load_model(model_args: ModelArgs):\n",
    "    model = FlagAutoModel.from_finetuned(\n",
    "        model_name_or_path=model_args.embedder_name_or_path,\n",
    "        model_class=model_args.embedder_model_class,\n",
    "        normalize_embeddings=model_args.normalize_embeddings,\n",
    "        pooling_method=model_args.pooling_method,\n",
    "        use_fp16=model_args.use_fp16,\n",
    "        query_instruction_for_retrieval=model_args.query_instruction_for_retrieval,\n",
    "        query_instruction_format=model_args.query_instruction_format_for_retrieval,\n",
    "        devices=model_args.devices,\n",
    "        examples_for_task=model_args.examples_for_task,\n",
    "        examples_instruction_format=model_args.examples_instruction_format,\n",
    "        trust_remote_code=model_args.trust_remote_code,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "        batch_size=model_args.batch_size,\n",
    "        query_max_length=model_args.embedder_query_max_length,\n",
    "        passage_max_length=model_args.embedder_passage_max_length,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def main(data_args: DataArgs, model_args: ModelArgs):\n",
    "    model = load_model(model_args)\n",
    "\n",
    "    find_knn_neg(\n",
    "        model=model,\n",
    "        input_file=data_args.input_file,\n",
    "        output_file=data_args.output_file,\n",
    "        candidate_pool=data_args.candidate_pool,\n",
    "        sample_range=[int(x) for x in data_args.range_for_sampling.split(\"-\")],\n",
    "        negative_number=data_args.negative_number,\n",
    "        use_gpu=data_args.use_gpu_for_searching,\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = HfArgumentParser((DataArgs, ModelArgs))\n",
    "    data_args, model_args = parser.parse_args_into_dataclasses()\n",
    "    data_args: DataArgs\n",
    "    model_args: ModelArgs\n",
    "    main(data_args, model_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f5ca1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 410,\n",
       " 'title': '제181조',\n",
       " 'query': '제181조(과실일수) 과실로 인하여 현주건조물 공용건조물 일반건조물 기차 전차 자동차 선박 항공기 또는 광갱을 침해하여 공공의 위험을 발생하게 한 자는 1천만원 이하의 벌금에 처한다',\n",
       " 'pos': ['피고인은 부주의하게 담배꽁초를 창문 밖으로 던져 아파트 화단에 불이 붙어 입주민들의 대피騒動이 발생한 사실이 있다.',\n",
       "  '피고인은 공사장에서 안전수칙을 미준수한 채 불꽃작업을 하다가 불씨가 건조물 내로 튀어 집기류에 불이 옮겨붙는 사고로 소방대가 출동했다.',\n",
       "  '피고인은 차량 정비 작업 시 배터리 탈거 중 스파크로 차량 내부에 불이 번져 인근 주차장의 다른 차량들까지 불이 번질 위험을 초래하였다.',\n",
       "  '피고인이 항공기 정비 도중 기체 주변에서 인화성 물질을 부주의하게 취급하여 조종실 부근에서 연소가 시작되어 탑승객 승·하차가 통제된 일이 있다.'],\n",
       " 'neg': ['동거가족에 해당하는 형제의 차량 소유에 대해 피고인이 무단으로 처분하여 권리행사를 방해하였으나, 동거 친족 관계이므로 형이 면제되었다.',\n",
       "  '피고인은 4명의 공범들과 함께 금속 파이프를 들고 회사 건물로 침입하여 유리문을 부수고 집기를 파손하였다.',\n",
       "  '피고인은 자신의 보호를 받는 만 15세의 아동을 위험한 건설 현장에서 잡일을 하도록 건설업자에게 넘겨주었다.',\n",
       "  '선상 강도단 일원인 피고인은 선장 가족을 위협하여 돈을 뺏고, 선장의 부인을 강간하여 피해자가 정신적·육체적으로 중상을 입게 하였다.',\n",
       "  '피고인은 경쟁 업체 대표가 불법 영업을 한다는 허위 사실을 지역 신문사에 제보하여 기사화되게 하여 명예를 훼손하였다.',\n",
       "  '피고인은 도로 위를 달리던 시외버스를 미리 준비한 장애물을 이용해 전복시켜 다수의 승객이 다치게 하였다.',\n",
       "  '철도 유지보수 업무를 담당하는 직원이 주행 중인 기차의 선로 이상을 발견하고도 즉시 조치하지 않아, 결국 기차가 전복되어 심각한 교통장애를 발생시켰다.',\n",
       "  '피고인은 아파트 가스계량기를 절단 및 제거함으로써 세대별 가스공급이 어려워지게 하였다.',\n",
       "  '피고인은 반대파 집회의 일환으로 고속도로에 타이어를 쌓아놓고 불을 질러 차량 진입을 불가능하게 하였다.',\n",
       "  '피고인은 동업자에게 협박 전화를 걸어 위협한 뒤 회사 금고의 돈 100만 원을 건네받았다.',\n",
       "  '피고인은 농수로용 댐의 배수시설을 고의적으로 차단, 폭우로 인해 인근 기차역 대합실에 물이 범람하여 대합실에 있던 1명이 중상해를 당했다.',\n",
       "  '피고는 해상에서 어선에 무단 침입하여 선장에게 둔기를 휘둘러 두개골 골절상을 입도록 하였다.',\n",
       "  'A씨는 경쟁 기업의 항공기 격납고를 대상으로 인근 저수지의 수문을 불법으로 개방해 대규모의 물을 건조물 내부로 유입시켜 항공기 및 격납고를 심각하게 손상시켰다.',\n",
       "  '세무 공무원이 없는 세금 체납 사실을 내세워 납세자에게 부당하게 행정제재 업무를 시켜 이행의무 없는 일을 하게 한 경우',\n",
       "  '시민단체 관계자인 피고인은 한 지방자치단체장의 비리 사실을 언론을 통해 발표하였고, 이로 인해 명예훼손으로 고소되었으나, 해당 내용이 사실이며 시민의 알 권리 보장을 위해 행해졌다.',\n",
       "  '피고인은 자신의 공장에 설치된 보일러의 안전장치를 고의로 해체한 후 운전하여 보일러가 폭발, 인근 주택가에 파편이 낙하해 주민들에게 위험을 초래하였다.',\n",
       "  '피고인은 야간에 주거에 침입하여 금품을 절취하던 중 이를 발견한 피해자가 소리를 지르자 체포를 면탈할 목적으로 피해자의 팔을 잡아 비틀며 밀치는 등 폭행을 가하였다.',\n",
       "  '피고인은 서울시내 운행 중이던 시내버스 내부에 불을 질러 공익을 위해 사용되는 자동차를 방화하였다.',\n",
       "  '피고인은 화재 현장에서 소방차가 사용하는 소화전을 고의로 파손하여 진화 활동을 어렵게 하였다.',\n",
       "  '피의자는 상대방에게 신체적 해를 가하겠다고 위협하여 자동차를 양도받았다.',\n",
       "  '철도공사 작업자가 안전규정을 위반한 채 선로 위에 장비를 방치하여, 그 장비에 기차가 부딪혀 전복되고 파괴되어 교통이 방해되었다.',\n",
       "  '피의자는 전원주택 단지 근처 농수로의 둑을 고의적으로 터뜨려 거주민이 현존하는 주택 3채가 침수 피해를 입게 했다.',\n",
       "  'A씨는 자신의 부주의로 인해 콘센트에 많은 전기제품을 동시에 연결하다 단락이 발생, 타인 소유의 원룸 건물에 화재가 발생하여 일부가 불에 탔다.',\n",
       "  '피고인은 자신 소유의 주택을 더 이상 유지하기 어렵다고 판단하여, 미리 준비한 성냥으로 주택 내부에 불을 질러 건물을 전소시킨 행위가 있다.',\n",
       "  '1. 피고인이 세관 공무원으로 근무하던 중, 직무상 봉함되어 보관되어야 할 과세자료를 무단으로 개봉하고 그 내용을 임의로 열람하였다.',\n",
       "  '피고인은 하천에 토사 및 폐기물을 투기하여 선박의 정상 운항이 불가능하게 하였다.',\n",
       "  '피고인은 피해자를 협박하여 피해자의 소지 금 500만원을 지급받았다.',\n",
       "  '피고인은 도심 지역에 위치한 변전실 담장을 넘어 침입한 뒤, 전기 공급을 담당하는 주요 분전반을 망치로 파손하여 해당 지역 일대에 대규모 정전 사태를 초래하였다.',\n",
       "  '피고인은 특정 업체와 사전에 담합해 낙찰 가격을 조정하고, 다른 입찰 참가자들의 입찰 참여를 방해하여 정상적인 입찰 절차를 저해하였다.',\n",
       "  '피고인은 시청 건설과 소속 공무원으로서, 지역 건설업체로부터 공사 수주를 대가로 1,000만원을 수수하기로 약속한 후 해당 업체에게 내부 정보를 유출하여 입찰에서 유리하게 하였다.',\n",
       "  '피의자는 화학공장 저장 탱크의 밸브를 몰래 열어 증기를 유출시키고 주변 차량 및 건물 재산에 손상을 가하였다.',\n",
       "  '피고인은 자신 명의 자동차 내부에 불을 질러 주차 중인 여러 주변 차량에 화재가 전이될 위험을 초래하였다.']}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "    \"id\": 410,\n",
    "    \"title\": \"제181조\",\n",
    "    \"query\": \"제181조(과실일수) 과실로 인하여 현주건조물 공용건조물 일반건조물 기차 전차 자동차 선박 항공기 또는 광갱을 침해하여 공공의 위험을 발생하게 한 자는 1천만원 이하의 벌금에 처한다\",\n",
    "    \"pos\": [\n",
    "        \"피고인은 부주의하게 담배꽁초를 창문 밖으로 던져 아파트 화단에 불이 붙어 입주민들의 대피騒動이 발생한 사실이 있다.\",\n",
    "        \"피고인은 공사장에서 안전수칙을 미준수한 채 불꽃작업을 하다가 불씨가 건조물 내로 튀어 집기류에 불이 옮겨붙는 사고로 소방대가 출동했다.\",\n",
    "        \"피고인은 차량 정비 작업 시 배터리 탈거 중 스파크로 차량 내부에 불이 번져 인근 주차장의 다른 차량들까지 불이 번질 위험을 초래하였다.\",\n",
    "        \"피고인이 항공기 정비 도중 기체 주변에서 인화성 물질을 부주의하게 취급하여 조종실 부근에서 연소가 시작되어 탑승객 승·하차가 통제된 일이 있다.\",\n",
    "    ],\n",
    "    \"neg\": [ # 32\n",
    "        \"동거가족에 해당하는 형제의 차량 소유에 대해 피고인이 무단으로 처분하여 권리행사를 방해하였으나, 동거 친족 관계이므로 형이 면제되었다.\",\n",
    "        \"피고인은 4명의 공범들과 함께 금속 파이프를 들고 회사 건물로 침입하여 유리문을 부수고 집기를 파손하였다.\",\n",
    "        # ...\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924f8497",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
