{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf0f625d",
   "metadata": {},
   "source": [
    "### Query - Pos 쌍에서 (relevant_incidents.jsonl 파일) 네거티브 샘플링 후 Train/Test 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6b48693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['query', 'pos', 'neg'],\n",
       "    num_rows: 762\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"json\", data_files=\"../data/relevant_incidents.jsonl\")['train']\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389a0cfe",
   "metadata": {},
   "source": [
    "### negative-mining.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1367b9a3",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/bash\n",
    "#SBATCH -J bge-m3\n",
    "#SBATCH --gres=gpu:1\n",
    "#SBATCH --cpus-per-gpu=6\n",
    "#SBATCH --mem-per-gpu=20G\n",
    "#SBATCH -w aurora-g3\n",
    "#SBATCH -p batch_ugrad\n",
    "#SBATCH -t 1-0\n",
    "#SBATCH -o logs/slurm-%A.out\n",
    "  \n",
    "pwd\n",
    "which python\n",
    "hostname\n",
    "\n",
    "python hn_mine.py \\\n",
    "--embedder_name_or_path BAAI/bge-m3 \\\n",
    "--input_file /data2/local_datasets/bge-m3/ft_data/relevant_incidents.jsonl \\\n",
    "--output_file /data2/local_datasets/bge-m3/ft_data/relevant_incidents_minedHN.jsonl \\\n",
    "--range_for_sampling 2-300 \\\n",
    "--negative_number 32 \\\n",
    "--use_gpu_for_searching \n",
    "\n",
    "exit 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ac3948",
   "metadata": {},
   "source": [
    "### hn_mine.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd86372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copied from https://github.com/FlagOpen/FlagEmbedding/blob/master/scripts/hn_mine.py\n",
    "\n",
    "# MIT License\n",
    "\n",
    "# Copyright (c) 2022 staoxiao\n",
    "\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "# of this software and associated documentation files (the \"Software\"), to deal\n",
    "# in the Software without restriction, including without limitation the rights\n",
    "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "# copies of the Software, and to permit persons to whom the Software is\n",
    "# furnished to do so, subject to the following conditions:\n",
    "\n",
    "# The above copyright notice and this permission notice shall be included in all\n",
    "# copies or substantial portions of the Software.\n",
    "\n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "# SOFTWARE.\n",
    "\n",
    "\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from typing import Optional\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "import faiss\n",
    "from transformers import HfArgumentParser\n",
    "from FlagEmbedding import FlagAutoModel\n",
    "from FlagEmbedding.abc.inference import AbsEmbedder\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataArgs:\n",
    "    \"\"\"\n",
    "    Data arguments for hard negative mining.\n",
    "    \"\"\"\n",
    "\n",
    "    input_file: str = field(\n",
    "        metadata={\"help\": \"The input file for hard negative mining.\"}\n",
    "    )\n",
    "    output_file: str = field(\n",
    "        metadata={\"help\": \"The output file for hard negative mining.\"}\n",
    "    )\n",
    "    candidate_pool: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"The candidate pool for hard negative mining. If provided, it should be a jsonl file, each line is a dict with a key 'text'.\"\n",
    "        },\n",
    "    )\n",
    "    range_for_sampling: str = field(\n",
    "        default=\"10-210\", metadata={\"help\": \"The range to sample negatives.\"}\n",
    "    )\n",
    "    negative_number: int = field(\n",
    "        default=15, metadata={\"help\": \"The number of negatives.\"}\n",
    "    )\n",
    "    use_gpu_for_searching: bool = field(\n",
    "        default=False, metadata={\"help\": \"Whether to use faiss-gpu for searching.\"}\n",
    "    )\n",
    "    search_batch_size: int = field(\n",
    "        default=64, metadata={\"help\": \"The batch size for searching.\"}\n",
    "    )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelArgs:\n",
    "    \"\"\"\n",
    "    Model arguments for embedder.\n",
    "    \"\"\"\n",
    "\n",
    "    embedder_name_or_path: str = field(\n",
    "        metadata={\"help\": \"The embedder name or path.\", \"required\": True}\n",
    "    )\n",
    "    embedder_model_class: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"The embedder model class. Available classes: ['encoder-only-base', 'encoder-only-m3', 'decoder-only-base', 'decoder-only-icl']. Default: None. For the custom model, you need to specifiy the model class.\",\n",
    "            \"choices\": [\n",
    "                \"encoder-only-base\",\n",
    "                \"encoder-only-m3\",\n",
    "                \"decoder-only-base\",\n",
    "                \"decoder-only-icl\",\n",
    "            ],\n",
    "        },\n",
    "    )\n",
    "    normalize_embeddings: bool = field(\n",
    "        default=True, metadata={\"help\": \"whether to normalize the embeddings\"}\n",
    "    )\n",
    "    pooling_method: str = field(\n",
    "        default=\"cls\", metadata={\"help\": \"The pooling method fot the embedder.\"}\n",
    "    )\n",
    "    use_fp16: bool = field(\n",
    "        default=True, metadata={\"help\": \"whether to use fp16 for inference\"}\n",
    "    )\n",
    "    devices: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Devices to use for inference.\", \"nargs\": \"+\"}\n",
    "    )\n",
    "    query_instruction_for_retrieval: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Instruction for query\"}\n",
    "    )\n",
    "    query_instruction_format_for_retrieval: str = field(\n",
    "        default=\"{}{}\", metadata={\"help\": \"Format for query instruction\"}\n",
    "    )\n",
    "    examples_for_task: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Examples for task\"}\n",
    "    )\n",
    "    examples_instruction_format: str = field(\n",
    "        default=\"{}{}\", metadata={\"help\": \"Format for examples instruction\"}\n",
    "    )\n",
    "    trust_remote_code: bool = field(\n",
    "        default=False, metadata={\"help\": \"Trust remote code\"}\n",
    "    )\n",
    "    cache_dir: str = field(\n",
    "        default=None, metadata={\"help\": \"Cache directory for models.\"}\n",
    "    )\n",
    "    # ================ for inference ===============\n",
    "    batch_size: int = field(\n",
    "        default=3000, metadata={\"help\": \"Batch size for inference.\"}\n",
    "    )\n",
    "    embedder_query_max_length: int = field(\n",
    "        default=512, metadata={\"help\": \"Max length for query.\"}\n",
    "    )\n",
    "    embedder_passage_max_length: int = field(\n",
    "        default=512, metadata={\"help\": \"Max length for passage.\"}\n",
    "    )\n",
    "\n",
    "    def __post_init__(self):\n",
    "        # replace \"\\\\n\" with \"\\n\"\n",
    "        if \"\\\\n\" in self.query_instruction_format_for_retrieval:\n",
    "            self.query_instruction_format_for_retrieval = (\n",
    "                self.query_instruction_format_for_retrieval.replace(\"\\\\n\", \"\\n\")\n",
    "            )\n",
    "        if \"\\\\n\" in self.examples_instruction_format:\n",
    "            self.examples_instruction_format = self.examples_instruction_format.replace(\n",
    "                \"\\\\n\", \"\\n\"\n",
    "            )\n",
    "\n",
    "\n",
    "def create_index(embeddings: np.ndarray, use_gpu: bool = False):\n",
    "    index = faiss.IndexFlatIP(len(embeddings[0]))\n",
    "    embeddings = np.asarray(embeddings, dtype=np.float32)\n",
    "    if use_gpu:\n",
    "        co = faiss.GpuMultipleClonerOptions()\n",
    "        co.shard = True\n",
    "        co.useFloat16 = True\n",
    "        index = faiss.index_cpu_to_all_gpus(index, co=co)\n",
    "    index.add(embeddings)\n",
    "    return index\n",
    "\n",
    "\n",
    "def batch_search(\n",
    "    index: faiss.Index, query: np.ndarray, topk: int = 200, batch_size: int = 64\n",
    "):\n",
    "    all_scores, all_inxs = [], []\n",
    "    for start_index in tqdm(\n",
    "        range(0, len(query), batch_size), desc=\"Batches\", disable=len(query) < 256\n",
    "    ):\n",
    "        batch_query = query[start_index : start_index + batch_size]\n",
    "        batch_scores, batch_inxs = index.search(\n",
    "            np.asarray(batch_query, dtype=np.float32), k=topk\n",
    "        )\n",
    "        all_scores.extend(batch_scores.tolist())\n",
    "        all_inxs.extend(batch_inxs.tolist())\n",
    "    return all_scores, all_inxs\n",
    "\n",
    "\n",
    "def get_corpus(candidate_pool: str):\n",
    "    corpus = []\n",
    "    with open(candidate_pool, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f.readlines():\n",
    "            line = json.loads(line.strip())\n",
    "            corpus.append(line[\"text\"])\n",
    "    return corpus\n",
    "\n",
    "\n",
    "def find_knn_neg(\n",
    "    model: AbsEmbedder,\n",
    "    input_file: str,\n",
    "    output_file: str,\n",
    "    candidate_pool: Optional[str] = None,\n",
    "    sample_range: str = \"10-210\",\n",
    "    negative_number: int = 15,\n",
    "    use_gpu: bool = False,\n",
    "):\n",
    "    corpus = []\n",
    "    queries = []\n",
    "    train_data = []\n",
    "    for line in open(input_file):\n",
    "        line = json.loads(line.strip())\n",
    "        train_data.append(line)\n",
    "        corpus.extend(line[\"pos\"])\n",
    "        if \"neg\" in line:\n",
    "            corpus.extend([line[\"neg\"]])\n",
    "        queries.append(line[\"query\"])\n",
    "\n",
    "    if candidate_pool is not None:\n",
    "        if not isinstance(candidate_pool, list):\n",
    "            candidate_pool = get_corpus(candidate_pool)\n",
    "        corpus = list(set(candidate_pool))\n",
    "    else:\n",
    "        corpus = list(set(corpus))\n",
    "\n",
    "    print(f\"inferencing embedding for corpus (number={len(corpus)})--------------\")\n",
    "    p_vecs = model.encode(corpus)\n",
    "    print(f\"inferencing embedding for queries (number={len(queries)})--------------\")\n",
    "    q_vecs = model.encode_queries(queries)\n",
    "\n",
    "    # check if the embeddings are in dictionary format: M3Embedder\n",
    "    if isinstance(p_vecs, dict):\n",
    "        p_vecs = p_vecs[\"dense_vecs\"]\n",
    "    if isinstance(q_vecs, dict):\n",
    "        q_vecs = q_vecs[\"dense_vecs\"]\n",
    "\n",
    "    print(\"create index and search------------------\")\n",
    "    index = create_index(p_vecs, use_gpu=use_gpu)\n",
    "    _, all_inxs = batch_search(index, q_vecs, topk=sample_range[-1])\n",
    "    assert len(all_inxs) == len(train_data)\n",
    "\n",
    "    for i, data in enumerate(train_data):\n",
    "        query = data[\"query\"]\n",
    "        inxs = all_inxs[i][sample_range[0] : sample_range[1]]\n",
    "        filtered_inx = []\n",
    "        for inx in inxs:\n",
    "            if inx == -1:\n",
    "                break\n",
    "            if corpus[inx] not in data[\"pos\"] and corpus[inx] != query:\n",
    "                filtered_inx.append(inx)\n",
    "\n",
    "        if len(filtered_inx) > negative_number:\n",
    "            filtered_inx = random.sample(filtered_inx, negative_number)\n",
    "        data[\"neg\"] = [corpus[inx] for inx in filtered_inx]\n",
    "\n",
    "    with open(output_file, \"w\") as f:\n",
    "        for data in train_data:\n",
    "            if len(data[\"neg\"]) < negative_number:\n",
    "                samples = random.sample(\n",
    "                    corpus, negative_number - len(data[\"neg\"]) + len(data[\"pos\"])\n",
    "                )\n",
    "                samples = [sent for sent in samples if sent not in data[\"pos\"]]\n",
    "                data[\"neg\"].extend(samples[: negative_number - len(data[\"neg\"])])\n",
    "            f.write(json.dumps(data, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "\n",
    "def load_model(model_args: ModelArgs):\n",
    "    model = FlagAutoModel.from_finetuned(\n",
    "        model_name_or_path=model_args.embedder_name_or_path,\n",
    "        model_class=model_args.embedder_model_class,\n",
    "        normalize_embeddings=model_args.normalize_embeddings,\n",
    "        pooling_method=model_args.pooling_method,\n",
    "        use_fp16=model_args.use_fp16,\n",
    "        query_instruction_for_retrieval=model_args.query_instruction_for_retrieval,\n",
    "        query_instruction_format=model_args.query_instruction_format_for_retrieval,\n",
    "        devices=model_args.devices,\n",
    "        examples_for_task=model_args.examples_for_task,\n",
    "        examples_instruction_format=model_args.examples_instruction_format,\n",
    "        trust_remote_code=model_args.trust_remote_code,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "        batch_size=model_args.batch_size,\n",
    "        query_max_length=model_args.embedder_query_max_length,\n",
    "        passage_max_length=model_args.embedder_passage_max_length,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def main(data_args: DataArgs, model_args: ModelArgs):\n",
    "    model = load_model(model_args)\n",
    "\n",
    "    find_knn_neg(\n",
    "        model=model,\n",
    "        input_file=data_args.input_file,\n",
    "        output_file=data_args.output_file,\n",
    "        candidate_pool=data_args.candidate_pool,\n",
    "        sample_range=[int(x) for x in data_args.range_for_sampling.split(\"-\")],\n",
    "        negative_number=data_args.negative_number,\n",
    "        use_gpu=data_args.use_gpu_for_searching,\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = HfArgumentParser((DataArgs, ModelArgs))\n",
    "    data_args, model_args = parser.parse_args_into_dataclasses()\n",
    "    data_args: DataArgs\n",
    "    model_args: ModelArgs\n",
    "    main(data_args, model_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f72a3374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c278ec4bc82b4806b5c3c9d8ee21c98f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['query', 'pos', 'neg'],\n",
       "    num_rows: 762\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"json\", data_files=\"../data/relevant_incidents_minedHN.jsonl\")[\"train\"]\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aafe4573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 42 : 0\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\"json\", data_files=\"../data/relevant_incidents.jsonl\")[\"train\"]\n",
    "\n",
    "k = len(ds[0]['pos'])\n",
    "for i, row in enumerate(ds):\n",
    "    if k != len(row['pos']):\n",
    "        print(f\"index {i} : {len(row['pos'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87664e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_dataset(ds):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf024648",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
