v1: 1 sentence attention layer; Self-Attention over query and passage embeddings
BGEM3 SAE model

v2: using weighted average of  sentence embeddings as category embeddings (Embedding of categories)
BGEM3 WAE model

v3: 8 sentence attention layers; Self-Attention over query and passage embeddings
BGEM3 SAE model

v4: 8 sentence attention layers; Self-Attention only over query embeddings
BGEM3 SAQE modelss